import asyncio
import json

import pdfkit
import fitz  # PyMuPDF
import markdown2
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter


from pdfç²¾è¯».utils.json_util import repair_json_output

llm = ChatOpenAI(model="deepseek-v3")
# llm2 = ChatOpenAI(model="gpt-4o-mini")
llm2 = ChatOpenAI(model="claude-sonnet-4-20250514")
# llm2 = ChatOpenAI(model="deepseek-reasoner-all")

def load_pdf_node(state: dict) -> dict:
    pdf_path = state["pdf_path"]
    doc = fitz.open(pdf_path)
    text = "\n".join([page.get_text() for page in doc])
    metadata = doc.metadata
    return {
        # **state,
        "paper_text": text,
        "metadata": metadata
    }


with open("prompts/split.txt", "r", encoding="utf-8") as f:
    SPLIT_PROMPT = f.read()

# æ§åˆ¶å¹¶å‘æ‰¹é‡ï¼ˆé˜²æ­¢ API è¶…é™ï¼‰
semaphore = asyncio.Semaphore(5)

async def process_chunk_limited(chunk, prompt_template, llm):
    async with semaphore:
        return await process_chunk(chunk, prompt_template, llm)

async def process_chunk(chunk: str, prompt_template: str, llm) -> list[dict]:
    prompt = PromptTemplate.from_template(prompt_template)
    try:
        response = await llm.ainvoke(prompt.format(text=chunk))
        content = repair_json_output(response.content)
        sections = json.loads(content)
        return sections
    except Exception as e:
        print(f"[âš ï¸ è­¦å‘Š] åˆ†å—å¤„ç†å¤±è´¥: {e}")
        print("\n====================æœ‰é—®é¢˜çš„chunkå†…å®¹ï¼š\n",chunk)
        return []

def section_split_node(state: dict) -> dict:
    import nest_asyncio
    nest_asyncio.apply()  # é€‚ç”¨äº Jupyter ç­‰ REPL ç¯å¢ƒ

    text = state["paper_text"]

    # ä½¿ç”¨claudeè¾“å…¥å…¨æ–‡
    prompt = PromptTemplate.from_template(SPLIT_PROMPT)
    response = llm2.invoke(prompt.format(text=text))
    content = repair_json_output(response.content)
    all_sections_lists = json.loads(content)

    # # 1. è‡ªåŠ¨åˆ†å—ï¼ˆé¿å…æˆªæ–­é‡è¦ç« èŠ‚ï¼‰
    # splitter = RecursiveCharacterTextSplitter(chunk_size=7000, chunk_overlap=2000)
    # chunks = splitter.split_text(text)

    # # é¡ºåºæ‰§è¡Œllmï¼Œå¤ªæ…¢äº†
    # all_sections = []
    # for i, chunk in enumerate(chunks):
    #     print(f"â³ æ­£åœ¨å¤„ç†ç¬¬{i+1}/{len(chunks)}å—")
    #     prompt = PromptTemplate.from_template(SPLIT_PROMPT)
    #     response = llm2.invoke(prompt.format(text=chunk))
    #     try:
    #         content = repair_json_output(response.content)
    #         sections = json.loads(content)
    #     except json.JSONDecodeError:
    #         print(f"[è­¦å‘Š] ç¬¬{i+1}å—è§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹ï¼š", response.content)
    #         continue
    #     all_sections.extend(sections)





# 2. åˆå¹¶é‡å¤æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰



    # print(f"ğŸ“„ å…±éœ€å¤„ç† {len(chunks)} ä¸ªå—ï¼Œæ­£åœ¨å¹¶è¡Œè°ƒç”¨ LLM...")
    #
    # # # å¹¶è¡Œè°ƒåº¦æ‰€æœ‰ chunk    #
    # tasks = [
    #     process_chunk_limited(chunk, SPLIT_PROMPT, llm)
    #     for chunk in chunks
    # ]
    # all_sections_lists = asyncio.run(asyncio.gather(*tasks))
    #
    # # å¹³é“ºæ‰€æœ‰å—çš„è¾“å‡ºï¼ˆå»æ‰ç©ºçš„ï¼‰
    all_sections = [sec for section_list in all_sections_lists for sec in section_list]
    #

    merged = {}
    for sec in all_sections:
        title = sec["title"].strip().capitalize()
        if title in merged:
            merged[title] += "\n" + sec["content"].strip()
        else:
            merged[title] = sec["content"].strip()

    final_sections = [{"title": k, "content": v} for k, v in merged.items()]
    print(f"âœ… å®Œæˆç« èŠ‚åˆ†å‰²ï¼Œå…± {len(final_sections)} æ®µ")

    # debug ä¿å­˜
    with open("temp/final_sections.json", "w", encoding="utf-8") as f:
        json.dump(final_sections, f, ensure_ascii=False, indent=2)

    # debug è¯»å–
    # with open("temp/final_sections.json", "r", encoding="utf-8") as f:
    #     final_sections = json.load(f)


    return {
        # **state,
        "sections": final_sections
    }


with open("prompts/summary.txt", "r", encoding="utf-8") as f:
    SUMMARY_PROMPT = f.read()

async def summarize_sections_node(state: dict) -> dict:
    sections = state["sections"]
    summaries = []

    # semaphore = asyncio.Semaphore(5)
    #
    # async def summarize_section(section):
    #     async with semaphore:
    #         prompt = PromptTemplate.from_template(SUMMARY_PROMPT)
    #         try:
    #             response = await llm.ainvoke(prompt.format(
    #                 title=section["title"],
    #                 content=section["content"]
    #             ))
    #             return {
    #                 "title": section["title"],
    #                 "summary": response.content.strip(),
    #                 "content": section["content"]
    #             }
    #         except Exception as e:
    #             print(f"[âš ï¸ æ‘˜è¦å¤±è´¥] {section['title']} - {e}")
    #             return {
    #                 "title": section["title"],
    #                 "summary": "[æ‘˜è¦å¤±è´¥]",
    #                 "content": section["content"],
    #             }
    #
    # tasks = [summarize_section(sec) for sec in sections]
    # summaries = await asyncio.gather(*tasks)
    #
    # print("æ‘˜è¦èŠ‚ç‚¹ç»“æŸ")
    #
    # # debug ä¿å­˜
    # with open("temp/summaries.json", "w", encoding="utf-8") as f:
    #     json.dump(summaries, f, ensure_ascii=False, indent=2)

    # # debug è¯»å–
    with open("temp/summaries.json", "r", encoding="utf-8") as f:
        summaries = json.load(f)

    return {
        # **state,
        "summaries": summaries
    }



with open("prompts/glossary.txt", "r", encoding="utf-8") as f:
    GLOSSARY_PROMPT = f.read()

async def extract_glossary_node(state: dict) -> dict:
    summaries = state["summaries"]
    # semaphore = asyncio.Semaphore(5)
    #
    # async def extract_glossary(section):
    #     async with semaphore:
    #         try:
    #             prompt_template = PromptTemplate.from_template(GLOSSARY_PROMPT)
    #             response = await llm.ainvoke(prompt_template.format(
    #                 title=section["title"],
    #                 content=section["content"],
    #                 summary = section["summary"],
    #             ))
    #             return json.loads(repair_json_output(response.content))
    #         except Exception as e:
    #             print(f"[âš ï¸æœ¯è¯­æå–å¤±è´¥] {section['title']} - {e}")
    #             return []
    #
    # tasks = [extract_glossary(sec) for sec in summaries]
    # glossary_list = await asyncio.gather(*tasks)
    # raw_glossary = [item for sublist in glossary_list for item in sublist]  # æ‰å¹³åŒ–
    #
    # # ä¸“ä¸šæœ¯è¯­å»é‡
    # seen = set()
    # glossary = []
    # for item in raw_glossary:
    #     term = item.get("term", "").strip()
    #     if not term:
    #         continue
    #     normalized_term = term.capitalize()
    #     if normalized_term not in seen:
    #         seen.add(normalized_term)
    #         glossary.append(item)
    #
    #
    # # debug ä¿å­˜
    # with open("temp/glossary.json", "w", encoding="utf-8") as f:
    #     json.dump(glossary, f, ensure_ascii=False, indent=2)

    # # debug è¯»å–
    with open("temp/glossary.json", "r", encoding="utf-8") as f:
        glossary = json.load(f)

    return {
        # **state,
        "glossary": glossary
    }


with open("prompts/insights.txt", "r", encoding="utf-8") as f:
    INSIGHTS_PROMPT = f.read()

async def extract_insights_node(state: dict) -> dict:
    summaries = state["summaries"]
    # semaphore = asyncio.Semaphore(5)
    #
    # async def extract_insights(section):
    #     async with semaphore:
    #         try:
    #             prompt_template = PromptTemplate.from_template(INSIGHTS_PROMPT)
    #             response = await llm.ainvoke(prompt_template.format(
    #                 title=section["title"],
    #                 summary=section["summary"],
    #                 content=section["content"],
    #             ))
    #             return [response.content]
    #         except Exception as e:
    #             print(f"[âš ï¸æå– insight å¤±è´¥] {section['title']} - {e}")
    #             return []
    #
    # tasks = [extract_insights(sec) for sec in summaries]
    # insight_list = await asyncio.gather(*tasks)
    # insights = [ins for sublist in insight_list for ins in sublist]
    #
    # with open("temp/insights.json", "w", encoding="utf-8") as f:
    #     json.dump(insights, f, ensure_ascii=False, indent=2)

    # # debug è¯»å–
    with open("temp/insights.json", "r", encoding="utf-8") as f:
        insights = json.load(f)

    return {
        # **state,
        "insights": insights
    }


with open("prompts/questions.txt", "r", encoding="utf-8") as f:
    QUESTIONS_PROMPT = f.read()

async def generate_questions_node(state: dict) -> dict:
    summaries = state["summaries"]

    # semaphore = asyncio.Semaphore(5)
    #
    # async def gen_questions(section):
    #     async with semaphore:
    #         try:
    #             prompt_template = PromptTemplate.from_template(QUESTIONS_PROMPT)
    #             response = await llm.ainvoke(prompt_template.format(
    #                 title=section["title"],
    #                 summary=section["summary"],
    #                 content=section["content"],
    #             ))
    #             return [response.content]
    #         except Exception as e:
    #             print(f"[âš ï¸æé—®ç”Ÿæˆå¤±è´¥] {section['title']} - {e}")
    #             return []
    #
    # tasks = [gen_questions(sec) for sec in summaries]
    # questions_list = await asyncio.gather(*tasks)
    # questions = [q for sublist in questions_list for q in sublist]
    #
    # with open("temp/questions.json", "w", encoding="utf-8") as f:
    #     json.dump(questions, f, ensure_ascii=False, indent=2)

    # # debug è¯»å–
    with open("temp/questions.json", "r", encoding="utf-8") as f:
        questions = json.load(f)

    return {
        # **state,
        "questions": questions
    }


def generate_final_report(state: dict) -> str:
    summaries = state.get("summaries", [])
    glossary = state.get("glossary", [])
    insights = state.get("insights", [])
    questions = state.get("questions", [])
    paper_title = state["paper_title"]
    report = [f"# ğŸ“˜ ç²¾è¯»æŠ¥å‘Šï¼š{paper_title}", ""]

    # ä¸€ã€æ‘˜è¦
    report.append("## ğŸ“ ä¸€ã€ç« èŠ‚æ‘˜è¦\n")
    for sec in summaries:
        report.append(f"### {sec['title']}\n{sec['summary']}\n")

    # äºŒã€æœ¯è¯­è¡¨
    report.append("## ğŸ§  äºŒã€æœ¯è¯­è¡¨ï¼ˆGlossaryï¼‰\n")
    report.append("| æœ¯è¯­ | è§£é‡Š |\n|------|------|")
    for item in glossary:
        report.append(f"| {item['term']} | {item['definition']} |")

    # ä¸‰ã€æ ¸å¿ƒæ´å¯Ÿ
    report.append("\n## ğŸ’¡ ä¸‰ã€æ ¸å¿ƒæ´å¯Ÿï¼ˆInsightsï¼‰\n")
    for insight in insights:
        report.append(f"- {insight}")

    # å››ã€é˜…è¯»é—®é¢˜
    report.append("\n## â“ å››ã€æ¨èé˜…è¯»é—®é¢˜ï¼ˆQuestionsï¼‰\n")
    for q in questions:
        report.append(f"- {q}")

    return "\n".join(report)

def save_report_as_pdf(state: dict):
    markdown_text = generate_final_report(state)
    output_path = "report.pdf"
    html_content = markdown2.markdown(markdown_text)

    # æ·»åŠ ç®€å•æ ·å¼ç¾åŒ– PDF
    html_template = f"""
    <html>
    <head>
    <meta charset="utf-8">
    <style>
        body {{ font-family: 'Arial', sans-serif; line-height: 1.6; margin: 2em; }}
        h1 {{ color: #2c3e50; }}
        h2 {{ color: #34495e; margin-top: 1.5em; }}
        table {{ border-collapse: collapse; width: 100%; }}
        table, th, td {{ border: 1px solid #ccc; padding: 8px; }}
        th {{ background-color: #f4f4f4; }}
    </style>
    </head>
    <body>
    {html_content}
    </body>
    </html>
    """

    options = {
        "encoding": "UTF-8",
        "page-size": "A4",
        "margin-top": "0.75in",
        "margin-right": "0.75in",
        "margin-bottom": "0.75in",
        "margin-left": "0.75in",
    }

    pdfkit.from_string(html_template, output_path, options=options)
    print(f"âœ… PDF saved to {output_path}")
    return {
        "output_path": output_path
    }